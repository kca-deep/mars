# AI ìˆ˜ìƒ‰ì§€ì—­ ì„ ì • MVP êµ¬í˜„ ê³„íšì„œ (v3.0 - í˜„í–‰í™”)

> ë³¸ ë¬¸ì„œëŠ” ì‚°ì•…ì§€ì—­ ì¡°ë‚œì ìˆ˜ìƒ‰ì„ ìœ„í•œ AI ê¸°ë°˜ Gridë³„ ì¡´ì¬ í™•ë¥  ì˜ˆì¸¡ ì‹œìŠ¤í…œì˜ ê¸°ìˆ  êµ¬í˜„ ê³„íšì„œì…ë‹ˆë‹¤.

**ë³€ê²½ ì´ë ¥**
| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ ë‚´ìš© |
|------|------|----------|
| 1.0 | 2026-01-16 | ì´ˆì•ˆ ì‘ì„± |
| 2.0 | 2026-01-16 | ì‹¤ì œ ê³µê³µë°ì´í„° ë°˜ì˜, GeoPandas ì¶”ê°€, í”¼ì²˜ ê³ ë„í™” |
| 3.0 | 2026-01-16 | **í˜„í–‰í™”** - í”„ë¡œë•ì…˜ í˜„í™© ë°˜ì˜, API 3ì¢… í†µí•©, í”„ë¡œì íŠ¸ êµ¬ì¡° ì •ë¹„ |

---

## 0. í”„ë¡œë•ì…˜ í˜„í™© (í˜„í–‰í™”)

### 0.1 í˜„ì¬ ìƒíƒœ
| í•­ëª© | ìƒíƒœ | ë¹„ê³  |
|------|------|------|
| ì†ŒìŠ¤ ì½”ë“œ | **ë¯¸êµ¬í˜„** | `src/` í´ë” ì—†ìŒ, ì‹ ê·œ êµ¬í˜„ í•„ìš” |
| í•™ìŠµ ëª¨ë¸ | ë¯¸ìƒì„± | `models/` í´ë” ì—†ìŒ |
| ë°ì´í„° | **ìˆ˜ì§‘ ì™„ë£Œ** | `data/` í´ë” ë‚´ CSV ë° API ë¬¸ì„œ ì¡´ì¬ |
| UI | ë¯¸êµ¬í˜„ | Streamlit ëŒ€ì‹œë³´ë“œ ì‹ ê·œ ê°œë°œ í•„ìš” |

### 0.2 ë³´ìœ  ë°ì´í„° í˜„í™©
```
data/
â”œâ”€â”€ ì†Œë°©ì²­_ì „êµ­ ì‚°ì•…ì‚¬ê³  í˜„í™©_20241231.csv           # 10,134ê±´
â”œâ”€â”€ ì†Œë°©ì²­_ì „êµ­ ì‚°ì•…ì‚¬ê³  êµ¬ì¡°í™œë™í˜„í™©_20201231.csv    # 13,189ê±´
â”œâ”€â”€ 03_ì‚°ì•…ê¸°ìƒì •ë³´_ê¸°ìˆ ë¬¸ì„œ_v1.5(ìˆ˜ì •ë³¸).docx       # ì‚°ì•…ê¸°ìƒ API ìŠ¤í™
â”œâ”€â”€ gateway_swagger_guide.pdf                       # ê³µê³µë°ì´í„° GW ê°€ì´ë“œ
â”œâ”€â”€ ì‚°ì•…ê´€ë ¨ api.md                                 # API ì •ë³´ í†µí•© ë¬¸ì„œ
â””â”€â”€ AI_ìˆ˜ìƒ‰ì§€ì—­_MVP_êµ¬í˜„ê³„íšì„œ.md                    # ë³¸ ë¬¸ì„œ
```

### 0.3 êµ¬í˜„ ìš°ì„ ìˆœìœ„
1. **Phase 1**: ë°ì´í„° íŒŒì´í”„ë¼ì¸ + API í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„
2. **Phase 2**: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ + ëª¨ë¸ í•™ìŠµ
3. **Phase 3**: Streamlit ëŒ€ì‹œë³´ë“œ UI
4. **Phase 4**: í‰ê°€ ë° ìµœì í™”

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 ëª©í‘œ
ì‚°ì•…ì§€ì—­ ì¡°ë‚œì ìˆ˜ìƒ‰ ì‹œ **100m x 100m Gridë³„ ì¡´ì¬ í™•ë¥ ì„ ì˜ˆì¸¡**í•˜ì—¬ ìš°ì„  ìˆ˜ìƒ‰ ì§€ì—­(Top-K)ì„ ì¶”ì²œí•˜ëŠ” AI ëª¨ë¸ ê°œë°œ

### 1.2 ê¸°ìˆ  ìŠ¤íƒ
| êµ¬ë¶„ | ê¸°ìˆ  | ë²„ì „ |
|------|------|------|
| ì–¸ì–´ | Python | 3.10+ |
| ML í”„ë ˆì„ì›Œí¬ | LightGBM | 4.0+ |
| ë°ì´í„° ì²˜ë¦¬ | Pandas, NumPy | 2.0+, 1.24+ |
| ì§€ë¦¬ ë°ì´í„° | GeoPandas, Shapely | 0.14+, 2.0+ |
| ì¢Œí‘œê³„ ë³€í™˜ | PyProj | 3.6+ |
| í‰ê°€/ê²€ì¦ | scikit-learn | 1.3+ |
| ì§€ë¦¬ ì—°ì‚° | GeoPy | 2.4+ |
| ì‹œê°í™” | Folium, Plotly | 0.15+, 5.18+ |
| API í†µì‹  | Requests, Tenacity | 2.31+, 8.2+ |
| ì›¹ UI | Streamlit | 1.29+ |

---

## 2. ì™¸ë¶€ API ì—°ë™ (3ì¢… í†µí•©)

### 2.1 API ëª©ë¡ ë° ì¸ì¦ ì •ë³´

| API | ì—”ë“œí¬ì¸íŠ¸ | API Key | ìš©ë„ |
|-----|-----------|---------|------|
| ì „íŒŒëˆ„ë¦¬ (ì´ë™í†µì‹  ê¸°ì§€êµ­) | `https://spectrummap.kr/openapiNew.do` | `l08p79fhk49yf50219g6` | RF ì»¤ë²„ë¦¬ì§€ |
| ì‚°ì•…ê¸°ìƒì •ë³´ | `https://apis.data.go.kr/1400377/mtweather` | `o0Z61WpQ8qc3mszVvN%2BVG4ijRUBzNkK%2Fy1AabKm6jM%2FNubQqwgisJnQANrPBQ8hvm3%2BBjiM84GyYVlKCAN9sqw%3D%3D` | ê¸°ìƒ ë°ì´í„° |
| ìœ„í—˜ì§€ì—­ POI | `https://apis.data.go.kr/B553662/dangerInfoService` | (ë™ì¼) | ìœ„í—˜ì§€ì—­ ì •ë³´ |

### 2.2 ì „íŒŒëˆ„ë¦¬ API (ì´ë™í†µì‹  ê¸°ì§€êµ­)

#### ìš”ì²­ íŒŒë¼ë¯¸í„°
| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|----------|-----|------|
| key | API_KEY | ì¸ì¦í‚¤ |
| searchId | 07 | ì‚°ì•…ì§€ì—­ ì´ë™í†µì‹  |
| SCH_CD | MOBILE | ê¸°ì§€êµ­ ì •ë³´ |
| PARK_CD | 1/2/3 | êµ­ë¦½/ë„ë¦½/êµ°ë¦½ |
| QUERY | ê³µì›ëª… ë˜ëŠ” ALL | ê²€ìƒ‰ ëŒ€ìƒ |
| CUS_CD | SK/KT/LG/ALL | í†µì‹ ì‚¬ |
| SERVICE_CD | 2G/3G/4G/5G/ALL | ì„œë¹„ìŠ¤ ìœ í˜• |
| pIndex | 1~ | í˜ì´ì§€ ë²ˆí˜¸ |
| pSize | 100 | í˜ì´ì§€ í¬ê¸° |

#### ì‘ë‹µ í•„ë“œ
```python
{
    "LAT": 37.xxxx,           # ê¸°ì§€êµ­ ìœ„ë„
    "LON": 128.xxxx,          # ê¸°ì§€êµ­ ê²½ë„
    "FRQ": 2100,              # ì£¼íŒŒìˆ˜ (MHz)
    "PWR": 20,                # ì¶œë ¥ (W)
    "ANT_FORM": "ì˜´ë‹ˆ",        # ì•ˆí…Œë‚˜ í˜•íƒœ
    "ANT_GAIN": 15,           # ì•ˆí…Œë‚˜ ì´ë“ (dBi)
    "SEA_ALT": 850,           # í•´ë°œê³  (m)
    "GRD_ALT": 30,            # ì§€ìƒê³  (m)
    "CUS_CD": "KT"            # í†µì‹ ì‚¬
}
```

### 2.3 ì‚°ì•…ê¸°ìƒì •ë³´ API

#### ìš”ì²­ íŒŒë¼ë¯¸í„°
| íŒŒë¼ë¯¸í„° | íƒ€ì… | í•„ìˆ˜ | ì„¤ëª… |
|----------|------|------|------|
| ServiceKey | string | Y | ê³µê³µë°ì´í„°í¬í„¸ ì¸ì¦í‚¤ (URL Encode) |
| pageNo | int | Y | í˜ì´ì§€ ë²ˆí˜¸ |
| numOfRows | int | Y | í•œ í˜ì´ì§€ ê²°ê³¼ ìˆ˜ |
| _type | string | N | ì‘ë‹µ í˜•ì‹ (xml/json) |
| localArea | string | N | ì§€ì—­ì½”ë“œ (01:ì„œìš¸, 02:ë¶€ì‚°...) |
| obsid | string | N | ê´€ì¸¡ì†Œë²ˆí˜¸ |
| tm | string | N | ê´€ì¸¡ì‹œê°„ (ì˜ˆ: 202103221952) |

#### ì‘ë‹µ í•„ë“œ
```python
{
    "obsid": "1910",              # ê´€ì¸¡ì†Œë²ˆí˜¸
    "obsname": "í™ì²œê´˜ë°©ì‚°ë´‰ìˆ˜ëŒ€", # ì‚° ì´ë¦„
    "localarea": "1",             # ì§€ì—­ì½”ë“œ
    "tm": "2021-06-30 18:09",     # ê´€ì¸¡ì‹œê°„
    "cprn": 10.4,                 # ëˆ„ì  ê°•ìˆ˜ëŸ‰ (mm)
    "rn": 10.5,                   # ë‹¹ì¼ëˆ„ì  ê°•ìˆ˜ëŸ‰ (mm)
    "hm10m": 67.5,                # 10m ìŠµë„ (%)
    "hm2m": 71.5,                 # 2m ìŠµë„ (%)
    "pa": 1000.7                  # ê¸°ì•• (hPa)
}
```

### 2.4 ìœ„í—˜ì§€ì—­ POI API

#### ì—”ë“œí¬ì¸íŠ¸ êµ¬ì¡°
```
Base URL: https://apis.data.go.kr/B553662/dangerInfoService
í˜¸ì¶œ ë°©ì‹: ê³µê³µë°ì´í„° GW í‘œì¤€ (Swagger ê¸°ë°˜)
```

#### ê³µí†µ íŒŒë¼ë¯¸í„°
| íŒŒë¼ë¯¸í„° | íƒ€ì… | í•„ìˆ˜ | ì„¤ëª… |
|----------|------|------|------|
| serviceKey | string | Y | ê³µê³µë°ì´í„°í¬í„¸ ì¸ì¦í‚¤ |
| pageNo | int | Y | í˜ì´ì§€ë²ˆí˜¸ |
| numOfRows | int | Y | í•œ í˜ì´ì§€ ê²°ê³¼ ìˆ˜ |
| returnType | string | N | ì‘ë‹µ íƒ€ì… (JSON/XML) |

### 2.5 í†µí•© API í´ë¼ì´ì–¸íŠ¸

```python
# src/data/api_clients.py

import requests
from typing import Optional, Dict, List
import pandas as pd
from tenacity import retry, stop_after_attempt, wait_exponential
from abc import ABC, abstractmethod
import urllib.parse

class BaseAPIClient(ABC):
    """API í´ë¼ì´ì–¸íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def _request(self, url: str, params: dict) -> dict:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ API ìš”ì²­"""
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        return response.json()

    @abstractmethod
    def fetch_data(self, **kwargs) -> pd.DataFrame:
        pass


class SpectrumMapClient(BaseAPIClient):
    """ì „íŒŒëˆ„ë¦¬ API í´ë¼ì´ì–¸íŠ¸ (ì´ë™í†µì‹  ê¸°ì§€êµ­)"""
    BASE_URL = "https://spectrummap.kr/openapiNew.do"

    def __init__(self, api_key: str = "l08p79fhk49yf50219g6"):
        self.api_key = api_key

    def fetch_data(
        self,
        park_name: str = "ALL",
        park_type: int = 1,  # 1=êµ­ë¦½, 2=ë„ë¦½, 3=êµ°ë¦½
        carrier: str = "ALL",
        service: str = "ALL"
    ) -> pd.DataFrame:
        """ì‚°ì•…ì§€ì—­ ì´ë™í†µì‹  ê¸°ì§€êµ­ ì •ë³´ ì¡°íšŒ"""
        all_data = []
        page = 1
        while True:
            params = {
                "key": self.api_key,
                "searchId": "07",
                "type": "json",
                "SCH_CD": "MOBILE",
                "PARK_CD": park_type,
                "QUERY": park_name,
                "CUS_CD": carrier,
                "SERVICE_CD": service,
                "pIndex": page,
                "pSize": 100
            }
            data = self._request(self.BASE_URL, params).get("data", [])
            if not data:
                break
            all_data.extend(data)
            page += 1
        return pd.DataFrame(all_data)


class MountainWeatherClient(BaseAPIClient):
    """ì‚°ì•…ê¸°ìƒì •ë³´ API í´ë¼ì´ì–¸íŠ¸"""
    BASE_URL = "https://apis.data.go.kr/1400377/mtweather/mountListSearch"

    def __init__(self, service_key: str):
        self.service_key = service_key

    def fetch_data(
        self,
        local_area: Optional[str] = None,
        obs_id: Optional[str] = None,
        obs_time: Optional[str] = None,
        num_of_rows: int = 100
    ) -> pd.DataFrame:
        """ì‚°ì•…ê¸°ìƒ ì •ë³´ ì¡°íšŒ"""
        all_data = []
        page = 1
        while True:
            params = {
                "ServiceKey": self.service_key,
                "pageNo": page,
                "numOfRows": num_of_rows,
                "_type": "json"
            }
            if local_area:
                params["localArea"] = local_area
            if obs_id:
                params["obsid"] = obs_id
            if obs_time:
                params["tm"] = obs_time

            result = self._request(self.BASE_URL, params)
            items = result.get("response", {}).get("body", {}).get("items", {}).get("item", [])

            if not items:
                break
            all_data.extend(items if isinstance(items, list) else [items])

            total_count = result.get("response", {}).get("body", {}).get("totalCount", 0)
            if page * num_of_rows >= total_count:
                break
            page += 1

        return pd.DataFrame(all_data)


class DangerInfoClient(BaseAPIClient):
    """ìœ„í—˜ì§€ì—­ POI API í´ë¼ì´ì–¸íŠ¸"""
    BASE_URL = "https://apis.data.go.kr/B553662/dangerInfoService"

    def __init__(self, service_key: str):
        self.service_key = service_key

    def fetch_data(
        self,
        endpoint: str = "getDangerInfoList",
        extra_params: Optional[dict] = None,
        num_of_rows: int = 100
    ) -> pd.DataFrame:
        """ìœ„í—˜ì§€ì—­ POI ì •ë³´ ì¡°íšŒ"""
        all_data = []
        page = 1
        while True:
            params = {
                "serviceKey": self.service_key,
                "pageNo": page,
                "numOfRows": num_of_rows,
                "returnType": "JSON"
            }
            if extra_params:
                params.update(extra_params)

            url = f"{self.BASE_URL}/{endpoint}"
            result = self._request(url, params)

            items = result.get("response", {}).get("body", {}).get("items", {}).get("item", [])
            if not items:
                break
            all_data.extend(items if isinstance(items, list) else [items])

            if len(items) < num_of_rows:
                break
            page += 1

        return pd.DataFrame(all_data)
```

---

## 3. ë°ì´í„° êµ¬ì¡°

### 3.1 ê³µê³µë°ì´í„° ìŠ¤í‚¤ë§ˆ (ë³´ìœ  ë°ì´í„°)

#### ì‚°ì•…ì‚¬ê³  í˜„í™© (10,134ê±´)
```
ì‹ ê³ ì¼ì        : DATE (YYYY-MM-DD)
ì‹ ê³ ì‹œê°        : TIME (HH:MM)
ë°œìƒì§€ì—­_ì‹œ     : ì‹œ/ë„
ë°œìƒì§€ì—­_êµ°     : ì‹œ/êµ°/êµ¬
ë°œìƒì§€ì—­_ì     : ì/ë©´/ë™
ë°œìƒì§€ì—­_ë¦¬     : ë¦¬/ê°€
ì‚¬ê³ ìœ í˜•        : ì¶”ë½/ì‹¤ì¡±/ì§ˆë³‘/íƒˆì§„ ë“±
ì‘ê¸‰ì²˜ì¹˜ì½”ë“œ    : ì‘ê¸‰ì²˜ì¹˜ ìœ í˜•
ì²˜ë¦¬ê²°ê³¼ì½”ë“œ    : ë³‘ì›ì´ì†¡/ë¶€ìƒì—†ìŒ/ì‚¬ë§ ë“±
êµ¬ì¡°ì¸ì›        : INTEGER
```

#### ì‚°ì•…ì‚¬ê³  êµ¬ì¡°í™œë™ (13,189ê±´)
```
ì‹ ê³ ì¼ì        : DATE
ì‹ ê³ ì‹œê°        : TIME
ì¶œë™ì¼ì        : DATE
ì¶œë™ì‹œê°        : TIME
ë°œìƒì§€ì—­_ì‹œ/êµ°/ì/ë¦¬ : ì§€ì—­ ì •ë³´
ì‚°ëª…            : ì‚° ì´ë¦„ (ì„¤ì•…ì‚°, ë¶í•œì‚° ë“±)
ì‚¬ê³ ìœ í˜•        : ìƒì„¸ ì‚¬ê³  ìœ í˜•
ì‘ê¸‰ì²˜ì¹˜ì½”ë“œ    : ì‘ê¸‰ì²˜ì¹˜ ìœ í˜•
êµ¬ì¡°ì¸ì›        : INTEGER
```

### 3.2 ì‚¬ê³  í†µê³„ ë¶„ì„ ê²°ê³¼

#### ì‚¬ê³ ìœ í˜•ë³„ ë¶„í¬ (13,189ê±´ ê¸°ì¤€)
| ì‚¬ê³ ìœ í˜• | ê±´ìˆ˜ | ë¹„ìœ¨ |
|----------|------|------|
| ê¸°íƒ€ì‚¬ê³  | 5,247 | 39.8% |
| ì¼ë°˜ì¶”ë½ | 3,355 | 25.4% |
| ì‹¤ì¡±ì¶”ë½ | 2,857 | 21.7% |
| ì§ˆë³‘í™˜ì | 1,044 | 7.9% |
| íƒˆì§„/íƒˆìˆ˜ | 566 | 4.3% |
| ê¸°íƒ€ | 120 | 0.9% |

#### ì‹œê°„ëŒ€ë³„ ë¶„í¬
```
ì‹œê°„ëŒ€    ê±´ìˆ˜    ë¹„ìœ¨
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
06-09ì‹œ   719    5.5%   â† ì´ë¥¸ ì•„ì¹¨
09-12ì‹œ  2,464   18.7%  â† ì˜¤ì „ ë“±ì‚°
12-15ì‹œ  4,045   30.7%  â† ì ì‹¬/ì˜¤í›„ í”¼í¬ â˜…
15-18ì‹œ  3,732   28.3%  â† ì˜¤í›„ í•˜ì‚°
18-21ì‹œ  1,756   13.3%  â† ì €ë…
21-06ì‹œ   473    3.6%   â† ì•¼ê°„ (ìœ„í—˜)
```

**ì¸ì‚¬ì´íŠ¸**:
- 12~15ì‹œ ì‚¬ê³  ì§‘ì¤‘ (30.7%) â†’ ì‹œê°„ëŒ€ í”¼ì²˜ ì¤‘ìš”
- ì•¼ê°„(21~06ì‹œ) ì‚¬ê³ ëŠ” ì ì§€ë§Œ ìœ„í—˜ë„ ë†’ìŒ

### 3.3 MVP ì…ë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ (ëª©í‘œ)

#### Episode Meta
```
episode_id      : ì‚¬ê±´ ê³ ìœ  ID
episode_type    : drill / real
agency          : ë‹´ë‹¹ ê¸°ê´€
region_code     : ì§€ì—­ ì½”ë“œ
start_time      : ìˆ˜ìƒ‰ ì‹œì‘ ì‹œê°
last_seen_time  : ë§ˆì§€ë§‰ ê´€ì¸¡ ì‹œê°
last_seen_lat   : ë§ˆì§€ë§‰ ê´€ì¸¡ ìœ„ë„
last_seen_lon   : ë§ˆì§€ë§‰ ê´€ì¸¡ ê²½ë„
```

#### Grid Definition (100m x 100m)
```
grid_id         : ê²©ì ID
center_lat/lon  : ê²©ì ì¤‘ì‹¬ ì¢Œí‘œ
grid_size_m     : ê²©ì í¬ê¸° (100m)
crs             : ì¢Œí‘œê³„ (EPSG:4326)
```

#### Ground Truth
```
episode_id      : FK â†’ episode_meta
gt_lat/lon      : ë°œê²¬ ìœ„ì¹˜
found_time      : ë°œê²¬ ì‹œê°
outcome         : found / not_found
```

---

## 4. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (7ê°œ ì¹´í…Œê³ ë¦¬)

### 4.1 í”¼ì²˜ ëª©ë¡

#### (A) ê±°ë¦¬ ê¸°ë°˜ í”¼ì²˜
```python
dist_to_last_seen          # Gridì™€ ë§ˆì§€ë§‰ ê´€ì¸¡ ìœ„ì¹˜ ê°„ ì§ì„  ê±°ë¦¬ (m)
dist_to_last_seen_log      # ë¡œê·¸ ë³€í™˜ ê±°ë¦¬
dist_to_last_seen_weighted # ì§€í˜• ê°€ì¤‘ ê±°ë¦¬
```

#### (B) RF ì‹ í˜¸ ê¸°ë°˜ í”¼ì²˜
```python
max_rssi_nearby            # ì£¼ë³€ ìµœëŒ€ RSSI
avg_rssi_nearby            # ì£¼ë³€ í‰ê·  RSSI
rssi_gradient              # RSSI ë³€í™”ìœ¨ (ë°©í–¥ ì¶”ì •)
observation_count          # RF ê´€ì¸¡ íšŸìˆ˜
avg_snr                    # í‰ê·  ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„
```

#### (C) ê¸°ì§€êµ­ ì»¤ë²„ë¦¬ì§€ í”¼ì²˜ (ì „íŒŒëˆ„ë¦¬ API)
```python
num_stations_in_range      # ì»¤ë²„ ê°€ëŠ¥í•œ ê¸°ì§€êµ­ ìˆ˜
dominant_carrier           # ì£¼ìš” í†µì‹ ì‚¬
avg_station_power          # í‰ê·  ê¸°ì§€êµ­ ì¶œë ¥ (W)
max_station_power          # ìµœëŒ€ ê¸°ì§€êµ­ ì¶œë ¥
coverage_score             # ì „íŒŒ ë„ë‹¬ ì˜ˆìƒ ì ìˆ˜
```

#### (D) ì§€í˜• í”¼ì²˜
```python
elevation_m                # ê³ ë„ (m)
slope_deg                  # ê²½ì‚¬ë„ (ë„)
landcover_encoded          # í† ì§€í”¼ë³µ (FOREST/GRASS/URBAN/WATER)
forest_density             # ì‚°ë¦¼ ë°€ë„ (0~1)
elevation_diff_from_last   # ë§ˆì§€ë§‰ ê´€ì¸¡ ëŒ€ë¹„ ê³ ë„ì°¨
```

#### (E) ì‹œê°„ í”¼ì²˜
```python
time_since_last_seen_min   # ë§ˆì§€ë§‰ ê´€ì¸¡ í›„ ê²½ê³¼ ì‹œê°„ (ë¶„)
hour_of_day                # ì‹œê°„ëŒ€ (0~23)
is_peak_hour               # í”¼í¬ ì‹œê°„ëŒ€ ì—¬ë¶€ (12~15ì‹œ)
is_night                   # ì•¼ê°„ ì—¬ë¶€ (21~06ì‹œ)
day_of_week                # ìš”ì¼
is_weekend                 # ì£¼ë§ ì—¬ë¶€
```

#### (F) ê¸°ìƒ í”¼ì²˜ (ì‚°ì•…ê¸°ìƒì •ë³´ API)
```python
precipitation_mm           # ê°•ìˆ˜ëŸ‰ (mm)
cumulative_rain_mm         # ëˆ„ì  ê°•ìˆ˜ëŸ‰ (mm)
humidity_2m                # 2m ìŠµë„ (%)
humidity_10m               # 10m ìŠµë„ (%)
pressure_hpa               # ê¸°ì•• (hPa)
weather_risk_score         # ê¸°ìƒ ìœ„í—˜ë„ ì¢…í•© ì ìˆ˜
is_rainy                   # ê°•ìš° ì—¬ë¶€
high_humidity              # ê³ ìŠµë„ ì—¬ë¶€ (>80%)
```

#### (G) ìœ„í—˜ì§€ì—­ í”¼ì²˜ (ìœ„í—˜ì§€ì—­ POI API)
```python
near_danger_zone           # ìœ„í—˜ì§€ì—­ ì¸ì ‘ ì—¬ë¶€
danger_zone_distance       # ê°€ì¥ ê°€ê¹Œìš´ ìœ„í—˜ì§€ì—­ê¹Œì§€ ê±°ë¦¬
danger_zone_type           # ìœ„í—˜ ìœ í˜• (ë‚™ì„, ê¸‰ê²½ì‚¬ ë“±)
historical_accident_count  # ê³¼ê±° ì‚¬ê³  ê±´ìˆ˜ (í†µê³„ ê¸°ë°˜)
```

### 4.2 í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì½”ë“œ

```python
# src/features/feature_engineering.py

import pandas as pd
import numpy as np
import geopandas as gpd
from geopy.distance import geodesic
from shapely.geometry import Point
from typing import Optional

class FeatureEngineer:
    def __init__(
        self,
        grid_df: pd.DataFrame,
        terrain_df: pd.DataFrame,
        station_df: pd.DataFrame,
        weather_df: Optional[pd.DataFrame] = None,
        danger_df: Optional[pd.DataFrame] = None
    ):
        self.grid_df = grid_df
        self.terrain_df = terrain_df
        self.station_df = station_df
        self.weather_df = weather_df
        self.danger_df = danger_df

        # GeoDataFrame ë³€í™˜
        self.grid_gdf = gpd.GeoDataFrame(
            grid_df,
            geometry=gpd.points_from_xy(grid_df['center_lon'], grid_df['center_lat']),
            crs="EPSG:4326"
        )

    def calculate_distance_features(self, episode_meta: pd.Series) -> pd.DataFrame:
        """ê±°ë¦¬ ê¸°ë°˜ í”¼ì²˜ ê³„ì‚°"""
        last_pos = (episode_meta['last_seen_lat'], episode_meta['last_seen_lon'])

        distances = []
        for _, grid in self.grid_df.iterrows():
            grid_pos = (grid['center_lat'], grid['center_lon'])
            dist = geodesic(last_pos, grid_pos).meters
            distances.append({
                'grid_id': grid['grid_id'],
                'dist_to_last_seen': dist,
                'dist_to_last_seen_log': np.log1p(dist)
            })

        return pd.DataFrame(distances)

    def calculate_coverage_features(self, coverage_radius_m: float = 2000) -> pd.DataFrame:
        """ê¸°ì§€êµ­ ì»¤ë²„ë¦¬ì§€ í”¼ì²˜ ê³„ì‚° (ì „íŒŒëˆ„ë¦¬ API ê¸°ë°˜)"""
        coverage_features = []

        for _, grid in self.grid_df.iterrows():
            grid_pos = (grid['center_lat'], grid['center_lon'])

            stations_in_range = []
            for _, station in self.station_df.iterrows():
                station_pos = (station['LAT'], station['LON'])
                if geodesic(grid_pos, station_pos).meters <= coverage_radius_m:
                    stations_in_range.append(station)

            if stations_in_range:
                stations_df = pd.DataFrame(stations_in_range)
                coverage_features.append({
                    'grid_id': grid['grid_id'],
                    'num_stations_in_range': len(stations_in_range),
                    'avg_station_power': stations_df['PWR'].mean(),
                    'max_station_power': stations_df['PWR'].max(),
                    'coverage_score': len(stations_in_range) * stations_df['PWR'].mean()
                })
            else:
                coverage_features.append({
                    'grid_id': grid['grid_id'],
                    'num_stations_in_range': 0,
                    'avg_station_power': 0,
                    'max_station_power': 0,
                    'coverage_score': 0
                })

        return pd.DataFrame(coverage_features)

    def calculate_weather_features(self, timestamp: str) -> pd.DataFrame:
        """ê¸°ìƒ í”¼ì²˜ ê³„ì‚° (ì‚°ì•…ê¸°ìƒì •ë³´ API ê¸°ë°˜)"""
        if self.weather_df is None or self.weather_df.empty:
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return pd.DataFrame({
                'grid_id': self.grid_df['grid_id'],
                'precipitation_mm': 0,
                'humidity_2m': 50,
                'pressure_hpa': 1013,
                'weather_risk_score': 0.5
            })

        weather_features = []
        for _, grid in self.grid_df.iterrows():
            # ê°€ì¥ ê°€ê¹Œìš´ ê´€ì¸¡ì†Œ ë°ì´í„° ì‚¬ìš© (ë‹¨ìˆœí™”)
            weather = self.weather_df.iloc[0] if len(self.weather_df) > 0 else {}

            precip = float(weather.get('rn', 0) or 0)
            humidity = float(weather.get('hm2m', 50) or 50)
            pressure = float(weather.get('pa', 1013) or 1013)

            # ê¸°ìƒ ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°
            risk_score = 0.3 * (precip / 50) + 0.4 * (humidity / 100) + 0.3 * ((1013 - pressure) / 50)
            risk_score = max(0, min(1, risk_score))

            weather_features.append({
                'grid_id': grid['grid_id'],
                'precipitation_mm': precip,
                'humidity_2m': humidity,
                'pressure_hpa': pressure,
                'weather_risk_score': risk_score,
                'is_rainy': 1 if precip > 0 else 0,
                'high_humidity': 1 if humidity > 80 else 0
            })

        return pd.DataFrame(weather_features)

    def calculate_time_features(self, episode_meta: pd.Series) -> pd.DataFrame:
        """ì‹œê°„ í”¼ì²˜ ê³„ì‚°"""
        from datetime import datetime

        last_seen = pd.to_datetime(episode_meta['last_seen_time'])
        current = pd.to_datetime(episode_meta.get('start_time', datetime.now()))

        time_diff_min = (current - last_seen).total_seconds() / 60
        hour = last_seen.hour
        day_of_week = last_seen.dayofweek

        return pd.DataFrame({
            'grid_id': self.grid_df['grid_id'],
            'time_since_last_seen_min': time_diff_min,
            'hour_of_day': hour,
            'is_peak_hour': 1 if 12 <= hour < 15 else 0,
            'is_night': 1 if hour >= 21 or hour < 6 else 0,
            'day_of_week': day_of_week,
            'is_weekend': 1 if day_of_week >= 5 else 0
        })

    def build_feature_matrix(self, episode_meta: pd.Series) -> pd.DataFrame:
        """ì „ì²´ í”¼ì²˜ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        # ê¸°ë³¸ Grid ì •ë³´
        feature_df = self.grid_df[['grid_id', 'center_lat', 'center_lon']].copy()

        # ê° í”¼ì²˜ ê·¸ë£¹ ê³„ì‚° ë° ë³‘í•©
        feature_df = feature_df.merge(
            self.terrain_df, on='grid_id', how='left'
        )
        feature_df = feature_df.merge(
            self.calculate_distance_features(episode_meta), on='grid_id', how='left'
        )
        feature_df = feature_df.merge(
            self.calculate_coverage_features(), on='grid_id', how='left'
        )
        feature_df = feature_df.merge(
            self.calculate_weather_features(episode_meta.get('last_seen_time', '')),
            on='grid_id', how='left'
        )
        feature_df = feature_df.merge(
            self.calculate_time_features(episode_meta), on='grid_id', how='left'
        )

        # í† ì§€í”¼ë³µ ì›í•« ì¸ì½”ë”©
        if 'landcover' in feature_df.columns:
            feature_df = pd.get_dummies(feature_df, columns=['landcover'], prefix='lc')

        return feature_df
```

---

## 5. ëª¨ë¸ í•™ìŠµ

### 5.1 LightGBM ì„¤ì • (MCP ë¬¸ì„œ ê¸°ë°˜)

```python
# src/models/train.py

import lightgbm as lgb
from sklearn.model_selection import GroupKFold
import numpy as np
import pandas as pd
from typing import Optional, Dict, List

class SearchAreaModel:
    def __init__(self):
        self.model = None
        self.feature_names = None
        self.params = {
            "boosting_type": "gbdt",
            "objective": "binary",
            "metric": ["binary_logloss", "auc"],
            "num_leaves": 31,
            "learning_rate": 0.05,
            "feature_fraction": 0.8,
            "bagging_fraction": 0.8,
            "bagging_freq": 5,
            "verbose": -1,
            "is_unbalance": True,  # ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬
            "seed": 42
        }

    def train(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: Optional[pd.DataFrame] = None,
        y_val: Optional[pd.Series] = None,
        num_boost_round: int = 100
    ):
        """ëª¨ë¸ í•™ìŠµ"""
        self.feature_names = X_train.columns.tolist()

        train_data = lgb.Dataset(X_train, label=y_train)
        valid_sets = [train_data]

        callbacks = [
            lgb.log_evaluation(period=20),
            lgb.early_stopping(stopping_rounds=10)
        ]

        if X_val is not None:
            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
            valid_sets.append(val_data)

        self.model = lgb.train(
            self.params,
            train_data,
            num_boost_round=num_boost_round,
            valid_sets=valid_sets,
            callbacks=callbacks
        )

        return self

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """Gridë³„ ì¡´ì¬ í™•ë¥  ì˜ˆì¸¡"""
        return self.model.predict(X)

    def get_top_k_grids(
        self,
        X: pd.DataFrame,
        grid_ids: List[str],
        k: int = 10
    ) -> pd.DataFrame:
        """ìƒìœ„ Kê°œ ìš°ì„  ìˆ˜ìƒ‰ ì§€ì—­ ë°˜í™˜"""
        probs = self.predict_proba(X)

        result = pd.DataFrame({
            'grid_id': grid_ids,
            'probability': probs
        })
        result = result.sort_values('probability', ascending=False)

        return result.head(k)

    def get_feature_importance(self) -> pd.DataFrame:
        """í”¼ì²˜ ì¤‘ìš”ë„ ë°˜í™˜"""
        importance = self.model.feature_importance(importance_type='gain')

        return pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)

    def cross_validate(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        groups: pd.Series,
        n_splits: int = 5
    ) -> Dict[str, float]:
        """GroupKFold êµì°¨ ê²€ì¦ (Episode ë‹¨ìœ„ ë¶„ë¦¬)"""
        gkf = GroupKFold(n_splits=n_splits)

        metrics = {'recall@10': [], 'recall@20': [], 'mrr': []}

        for train_idx, val_idx in gkf.split(X, y, groups):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            self.train(X_train, y_train)
            probs = self.predict_proba(X_val)

            # Recall@K ê³„ì‚°
            sorted_idx = np.argsort(probs)[::-1]
            y_val_sorted = y_val.iloc[sorted_idx].values

            total_positive = y_val.sum()
            if total_positive > 0:
                metrics['recall@10'].append(y_val_sorted[:10].sum() / total_positive)
                metrics['recall@20'].append(y_val_sorted[:20].sum() / total_positive)

            # MRR ê³„ì‚°
            positive_ranks = np.where(y_val_sorted == 1)[0] + 1
            mrr = (1 / positive_ranks[0]) if len(positive_ranks) > 0 else 0
            metrics['mrr'].append(mrr)

        return {k: np.mean(v) for k, v in metrics.items()}
```

### 5.2 ë¼ë²¨ ìƒì„±

```python
# src/models/label_generator.py

import pandas as pd
from geopy.distance import geodesic

def generate_labels(
    grid_df: pd.DataFrame,
    ground_truth: pd.Series,
    positive_radius_m: float = 150
) -> pd.DataFrame:
    """Ground Truth ê¸°ë°˜ ë¼ë²¨ ìƒì„± (ë°œê²¬ ì§€ì  ë°˜ê²½ ë‚´ Grid = 1)"""
    gt_pos = (ground_truth['gt_lat'], ground_truth['gt_lon'])

    labels = []
    for _, grid in grid_df.iterrows():
        grid_pos = (grid['center_lat'], grid['center_lon'])
        dist = geodesic(gt_pos, grid_pos).meters

        label = 1 if dist <= positive_radius_m else 0
        labels.append({
            'grid_id': grid['grid_id'],
            'label': label,
            'dist_to_gt': dist
        })

    return pd.DataFrame(labels)
```

---

## 6. í‰ê°€ ì²´ê³„

### 6.1 í‰ê°€ ì§€í‘œ

```python
# src/evaluation/metrics.py

import numpy as np

def recall_at_k(y_true: np.ndarray, y_pred_proba: np.ndarray, k: int) -> float:
    """ìƒìœ„ Kê°œ ì˜ˆì¸¡ ë‚´ ì •ë‹µ ë¹„ìœ¨"""
    sorted_idx = np.argsort(y_pred_proba)[::-1][:k]
    return y_true[sorted_idx].sum() / y_true.sum() if y_true.sum() > 0 else 0

def mrr(y_true: np.ndarray, y_pred_proba: np.ndarray) -> float:
    """Mean Reciprocal Rank"""
    sorted_idx = np.argsort(y_pred_proba)[::-1]
    y_sorted = y_true[sorted_idx]
    positive_ranks = np.where(y_sorted == 1)[0] + 1
    return (1 / positive_ranks[0]) if len(positive_ranks) > 0 else 0

def search_area_reduction_at_k(total_grids: int, k: int, recall_at_k_value: float) -> float:
    """ìˆ˜ìƒ‰ ë©´ì  ì¶•ì†Œìœ¨"""
    if recall_at_k_value == 0:
        return 0
    reduced_area = k / recall_at_k_value
    return 1 - (reduced_area / total_grids)

def evaluate_model(y_true: np.ndarray, y_pred_proba: np.ndarray, total_grids: int) -> dict:
    """ì¢…í•© í‰ê°€"""
    r10 = recall_at_k(y_true, y_pred_proba, 10)
    r20 = recall_at_k(y_true, y_pred_proba, 20)

    return {
        'recall@10': r10,
        'recall@20': r20,
        'mrr': mrr(y_true, y_pred_proba),
        'area_reduction@10': search_area_reduction_at_k(total_grids, 10, r10),
        'area_reduction@20': search_area_reduction_at_k(total_grids, 20, r20)
    }
```

### 6.2 ì„±ê³µ ê¸°ì¤€ (Go/No-Go)

| ì§€í‘œ | ë² ì´ìŠ¤ë¼ì¸ ì˜ˆìƒ | MVP ëª©í‘œ | Go ì¡°ê±´ |
|------|----------------|----------|---------|
| Recall@10 | ~0.3 | â‰¥0.5 | +20%p ì´ìƒ |
| Recall@20 | ~0.5 | â‰¥0.7 | +20%p ì´ìƒ |
| MRR | ~0.2 | â‰¥0.4 | +0.2 ì´ìƒ |
| ë©´ì  ì¶•ì†Œìœ¨@10 | ê¸°ì¤€ | â‰¥30% | ë‹¬ì„± ì‹œ Go |

---

## 7. Streamlit ëŒ€ì‹œë³´ë“œ (MCP ë¬¸ì„œ ê¸°ë°˜)

### 7.1 UI êµ¬ì„±
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI ìˆ˜ìƒ‰ì§€ì—­ ì„ ì • ì‹œìŠ¤í…œ                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [ì‚¬ì´ë“œë°”]                  â”‚  [ë©”ì¸ ì˜ì—­]                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ì—í”¼ì†Œë“œ ì„ íƒ   â”‚         â”‚  â”‚                         â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚  â”‚      Heatmap           â”‚  â”‚
â”‚  â”‚ â”‚ Dropdown  â”‚ â”‚         â”‚  â”‚      (Folium ì§€ë„)      â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚  â”‚                         â”‚  â”‚
â”‚  â”‚               â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚ ì„¤ì •          â”‚         â”‚                               â”‚
â”‚  â”‚ Top-K: [10]   â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ë°˜ê²½: [500]m  â”‚         â”‚  â”‚  Top-K ìˆ˜ìƒ‰ ìš°ì„ ì§€ì—­     â”‚  â”‚
â”‚  â”‚               â”‚         â”‚  â”‚  (í…Œì´ë¸” + ì°¨íŠ¸)         â”‚  â”‚
â”‚  â”‚ [ì˜ˆì¸¡ ì‹¤í–‰]   â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚               â”‚         â”‚                               â”‚
â”‚  â”‚ ëª¨ë¸ ì •ë³´     â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ - Recall@10   â”‚         â”‚  â”‚  í”¼ì²˜ ì¤‘ìš”ë„             â”‚  â”‚
â”‚  â”‚ - MRR         â”‚         â”‚  â”‚  (Bar Chart)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 Streamlit ì•± ì½”ë“œ

```python
# src/app/streamlit_app.py

import streamlit as st
import pandas as pd
import numpy as np
import folium
from streamlit_folium import st_folium
from folium.plugins import HeatMap
import plotly.express as px
from pathlib import Path
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent))

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI ìˆ˜ìƒ‰ì§€ì—­ ì„ ì • ì‹œìŠ¤í…œ",
    page_icon="ğŸ”",
    layout="wide"
)

@st.cache_data
def load_sample_data():
    """ìƒ˜í”Œ ë°ì´í„° ë¡œë”© (ì‹¤ì œ êµ¬í˜„ ì‹œ DataLoader ì‚¬ìš©)"""
    # Grid ë°ì´í„° ìƒ˜í”Œ
    np.random.seed(42)
    grid_df = pd.DataFrame({
        'grid_id': [f'GRID-100M-{i:04d}' for i in range(100)],
        'center_lat': 37.5 + np.random.randn(100) * 0.01,
        'center_lon': 127.0 + np.random.randn(100) * 0.01
    })
    return {'grid_df': grid_df}

def create_heatmap(predictions: pd.DataFrame, center: list, gt: dict = None):
    """Folium Heatmap ìƒì„±"""
    m = folium.Map(location=center, zoom_start=14)

    # Heatmap ë ˆì´ì–´
    heat_data = [
        [row['center_lat'], row['center_lon'], row['probability']]
        for _, row in predictions.iterrows()
    ]
    HeatMap(heat_data, radius=15, blur=10).add_to(m)

    # ë§ˆì§€ë§‰ ê´€ì¸¡ ìœ„ì¹˜
    folium.Marker(
        center,
        popup="ë§ˆì§€ë§‰ ê´€ì¸¡ ìœ„ì¹˜",
        icon=folium.Icon(color='blue', icon='info-sign')
    ).add_to(m)

    # Top-10 ë§ˆì»¤
    top_10 = predictions.head(10)
    for idx, row in top_10.iterrows():
        folium.CircleMarker(
            [row['center_lat'], row['center_lon']],
            radius=10,
            color='red',
            fill=True,
            popup=f"ìˆœìœ„: {idx+1}, í™•ë¥ : {row['probability']:.2%}"
        ).add_to(m)

    # Ground Truth
    if gt:
        folium.Marker(
            [gt['lat'], gt['lon']],
            popup="ì‹¤ì œ ë°œê²¬ ìœ„ì¹˜",
            icon=folium.Icon(color='green', icon='ok-sign')
        ).add_to(m)

    return m

def main():
    st.title("AI ìˆ˜ìƒ‰ì§€ì—­ ì„ ì • ì‹œìŠ¤í…œ")
    st.markdown("ì‚°ì•…ì§€ì—­ ì¡°ë‚œì ìˆ˜ìƒ‰ì„ ìœ„í•œ AI ê¸°ë°˜ ìš°ì„  ìˆ˜ìƒ‰ ì§€ì—­ ì¶”ì²œ")

    # ë°ì´í„° ë¡œë”©
    data = load_sample_data()

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ì„¤ì •")

        # íŒŒë¼ë¯¸í„° ì„¤ì •
        top_k = st.slider("Top-K (ì¶”ì²œ ì§€ì—­ ìˆ˜)", 5, 30, 10)

        st.divider()

        # ì˜ˆì¸¡ ì‹¤í–‰ ë²„íŠ¼
        run_prediction = st.button("ì˜ˆì¸¡ ì‹¤í–‰", type="primary", use_container_width=True)

        st.divider()

        # ëª¨ë¸ ì„±ëŠ¥
        st.header("ëª¨ë¸ ì„±ëŠ¥ (ëª©í‘œ)")
        col1, col2 = st.columns(2)
        col1.metric("Recall@10", "â‰¥0.50")
        col2.metric("MRR", "â‰¥0.40")

    # ë©”ì¸ ì˜ì—­
    if run_prediction:
        # ì˜ˆì¸¡ ì‹¤í–‰ (ë°ëª¨ìš©)
        predictions = data['grid_df'].copy()
        np.random.seed(42)
        predictions['probability'] = np.random.beta(2, 5, len(predictions))
        predictions = predictions.sort_values('probability', ascending=False).reset_index(drop=True)

        center = [predictions['center_lat'].mean(), predictions['center_lon'].mean()]

        # ë ˆì´ì•„ì›ƒ
        col_map, col_info = st.columns([2, 1])

        with col_map:
            st.subheader("ìˆ˜ìƒ‰ ìš°ì„ ìˆœìœ„ Heatmap")
            heatmap = create_heatmap(predictions, center)
            st_folium(heatmap, width=700, height=500)

        with col_info:
            st.subheader(f"Top-{top_k} ìˆ˜ìƒ‰ ìš°ì„ ì§€ì—­")
            top_k_df = predictions.head(top_k)[['grid_id', 'probability']].copy()
            top_k_df['probability'] = top_k_df['probability'].apply(lambda x: f"{x:.2%}")
            top_k_df.index = range(1, len(top_k_df) + 1)
            st.dataframe(top_k_df, use_container_width=True)

        # í”¼ì²˜ ì¤‘ìš”ë„ (ë°ëª¨)
        st.divider()
        st.subheader("í”¼ì²˜ ì¤‘ìš”ë„")
        feature_imp = pd.DataFrame({
            'feature': ['dist_to_last_seen', 'coverage_score', 'elevation_m',
                       'weather_risk_score', 'is_peak_hour', 'num_stations'],
            'importance': [0.35, 0.22, 0.18, 0.12, 0.08, 0.05]
        })
        fig = px.bar(feature_imp, x='importance', y='feature', orientation='h',
                    color='importance', color_continuous_scale='Reds')
        fig.update_layout(height=300, showlegend=False)
        st.plotly_chart(fig, use_container_width=True)

if __name__ == "__main__":
    main()
```

### 7.3 ì‹¤í–‰ ë°©ë²•
```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install streamlit streamlit-folium plotly folium

# ì•± ì‹¤í–‰
streamlit run src/app/streamlit_app.py

# ë¸Œë¼ìš°ì € ì ‘ì†
# http://localhost:8501
```

---

## 8. í”„ë¡œì íŠ¸ êµ¬ì¡° (í˜„í–‰í™”)

```
mars/
â”œâ”€â”€ src/                            # â˜… ì‹ ê·œ ìƒì„± í•„ìš”
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api_clients.py          # 3ì¢… API í†µí•© í´ë¼ì´ì–¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ data_loader.py          # ë°ì´í„° ë¡œë”©
â”‚   â”‚   â””â”€â”€ preprocessor.py         # ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ feature_engineering.py  # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ baseline.py             # ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ train.py                # LightGBM í•™ìŠµ
â”‚   â”‚   â””â”€â”€ label_generator.py      # ë¼ë²¨ ìƒì„±
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ metrics.py              # í‰ê°€ ì§€í‘œ
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ heatmap.py              # Heatmap ì‹œê°í™”
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ streamlit_app.py        # Streamlit ëŒ€ì‹œë³´ë“œ
â”œâ”€â”€ notebooks/                       # â˜… ì‹ ê·œ ìƒì„± í•„ìš”
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_model_training.ipynb
â”‚   â””â”€â”€ 04_evaluation.ipynb
â”œâ”€â”€ data/                            # âœ… ì¡´ì¬
â”‚   â”œâ”€â”€ ì†Œë°©ì²­_ì „êµ­ ì‚°ì•…ì‚¬ê³  í˜„í™©_20241231.csv
â”‚   â”œâ”€â”€ ì†Œë°©ì²­_ì „êµ­ ì‚°ì•…ì‚¬ê³  êµ¬ì¡°í™œë™í˜„í™©_20201231.csv
â”‚   â”œâ”€â”€ 03_ì‚°ì•…ê¸°ìƒì •ë³´_ê¸°ìˆ ë¬¸ì„œ_v1.5(ìˆ˜ì •ë³¸).docx
â”‚   â”œâ”€â”€ gateway_swagger_guide.pdf
â”‚   â”œâ”€â”€ ì‚°ì•…ê´€ë ¨ api.md
â”‚   â””â”€â”€ AI_ìˆ˜ìƒ‰ì§€ì—­_MVP_êµ¬í˜„ê³„íšì„œ.md
â”œâ”€â”€ models/                          # â˜… ì‹ ê·œ ìƒì„± í•„ìš”
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                  # API í‚¤ ë“± ì„¤ì •
â”œâ”€â”€ tests/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ CLAUDE.md                        # âœ… ì¡´ì¬
â””â”€â”€ README.md
```

---

## 9. ì˜ì¡´ì„± íŒ¨í‚¤ì§€

```txt
# requirements.txt

# Core
pandas>=2.0.0
numpy>=1.24.0

# ML
lightgbm>=4.0.0
scikit-learn>=1.3.0

# Geo
geopandas>=0.14.0
geopy>=2.4.0
shapely>=2.0.0
folium>=0.15.0
pyproj>=3.6.0

# API & Utils
requests>=2.31.0
pyyaml>=6.0
python-dotenv>=1.0.0
tenacity>=8.2.0

# Visualization
matplotlib>=3.7.0
plotly>=5.18.0

# Web UI
streamlit>=1.29.0
streamlit-folium>=0.15.0

# Data
openpyxl>=3.1.0

# Testing
pytest>=7.4.0
pytest-cov>=4.1.0
```

---

## 10. êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: ë°ì´í„° íŒŒì´í”„ë¼ì¸ (1ì£¼ì°¨)
- [ ] `src/` ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] API í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- [ ] CSV ë°ì´í„° ë¡œë” êµ¬í˜„
- [ ] ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

### Phase 2: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (2ì£¼ì°¨)
- [ ] Grid ìƒì„± ë¡œì§ êµ¬í˜„
- [ ] 7ê°œ ì¹´í…Œê³ ë¦¬ í”¼ì²˜ ê³„ì‚° í•¨ìˆ˜ êµ¬í˜„
- [ ] í”¼ì²˜ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ê²€ì¦

### Phase 3: ëª¨ë¸ í•™ìŠµ (3ì£¼ì°¨)
- [ ] ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬í˜„
- [ ] LightGBM í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- [ ] GroupKFold êµì°¨ ê²€ì¦ ì‹¤í–‰
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### Phase 4: UI ë° í‰ê°€ (4ì£¼ì°¨)
- [ ] Streamlit ëŒ€ì‹œë³´ë“œ êµ¬í˜„
- [ ] í‰ê°€ ì§€í‘œ êµ¬í˜„ ë° ë¦¬í¬íŠ¸ ìƒì„±
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ë° ë¬¸ì„œí™”

---

## 11. ë‹¤ìŒ ë‹¨ê³„ (MVP ì´í›„)

1. **ë°ì´í„° í™•ëŒ€**: ì‹¤ì œ êµ¬ì¡° ì‚¬ë¡€ ë°ì´í„° í™•ë³´
2. **ì‹¤ì‹œê°„ ì—°ë™**: RF ê´€ì¸¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
3. **ë“œë¡  ì—°ê³„**: ë“œë¡  ì´ë™êµ­ í†µí•©
4. **ë­í‚¹ ëª¨ë¸**: LambdaMART ë“± Learning-to-Rank ì ìš©
5. **ìš´ì˜ ì‹œìŠ¤í…œ**: API ì„œë²„ ë°°í¬

---

*ì‘ì„±ì¼: 2026-01-16*
*ë²„ì „: 3.0 (í˜„í–‰í™”)*
